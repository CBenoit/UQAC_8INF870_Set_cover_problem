%!TEX program = lualatex
\documentclass[12pt,letterpaper,twoside]{article}
\include{packages}
\include{settings}

\bibliography{references}
\nocite{*}

\begin{document}
	\maketitle{}
	\tableofcontents{}
	\newpage{}
	\section{Définition du problème}
		\subsection{SCP et WSCP}
			\paragraph*{Présentation\\}
				Le problème de couverture d'ensemble, ou \emph{Set Covering Problem} (SCP),
				fait parti des 21 problèmes NP-complets de \citeauthor{Karp1972}~\cite{Karp1972}
				et est NP-complet au sens fort \cite{garey2002computers}.
			\paragraph*{Problème de couverture d'ensemble\\}
				Étant donné un ensemble univers \(U = \{u_1, u_2, u_3, \dots, u_m\}\) et une famille \(S = \{s_1, s_2, \dots, s_n\}\) de sous-ensembles de \(U\),
				le problème consiste à trouver une sous-famille de \(S\) la plus petite possible permettant de couvrir chaque élément de \(U\)
				au moins une fois. Un élément \(e\) de \(U\) est couvert par un sous-ensemble \(A\) si \(e \in A\).
			\paragraph*{Problème de couverture d'ensemble pondéré\\}
				En associant un coût positif \(c_i\) à chaque sous-ensemble, on obtient le problème de couverture d'ensemble pondéré ou \emph{Weighted Set Covering Problem} (WSCP) et	l'objectif est alors de déterminer une couverture de coût minimum.~\cite{Vazirani2003}
		\subsection{Utilité}
			\paragraph*{}
				Une grande variété de problèmes de positionnement, de distribution, de planification et autres peuvent être formulés comme variantes du problème de couverture d'ensemble. Parmi les problèmes réels auxquels cette approche a été appliquée avec succès:\cite{Balas1982}
				\begin{itemize}
					\item problèmes de sélection de sites et d'allocation d'emplacement
					\item emplacement des installations des services d'urgence (casernes de pompiers, hôpitaux, etc.)
					\item choix de la taille et de l'emplacement des plates-formes de forage dans les champs pétrolifères en mer
					\item horaire des équipages pour les compagnies aériennes, les compagnies de bus, les chemins de fer
					\item répartition des fréquences de radiodiffusion entre stations de radio ou de télévision
					\item recherche d'informations (à partir de fichiers informatiques)
					\item \ldots
				\end{itemize}
	\newpage\section{Exemple minimal}
		\paragraph*{}
			Soit un ensemble univers \(U = \{u_1, u_2, \dots, u_{12}\}\) (représenté par des points sur le figure \ref{fig:example}) et une famille \(S = \{s_1, s_2, \dots, s_6\}\) de sous-ensembles de \(U\) (représentés par des rectangles sur la figure \ref{fig:example}) avec:
			\begin{itemize}
				\item \(s_1 = \{u_1, u_2, u_3, u_4, u_5, u_6\}\)
				\item \(s_2 = \{u_5, u_6, u_8, u_9\}\)
				\item \(s_3 = \{u_1, u_4, u_7, u_{10}\}\)
				\item \(s_4 = \{u_2, u_5, u_8, u_{11}\}\)
				\item \(s_5 = \{u_3, u_6, u_9, u_{12}\}\)
				\item \(s_6 = \{u_{10}, u_{11}, u_{12}\}\)
			\end{itemize}
		\paragraph*{}
			La solution optimale à cette instance est la sous famille \(S'=\{s_3, s_4, s_5\}\) (colorée en gris sur la figure \ref{fig:example}).
		\begin{figure}[H]
			\centering%
			\includegraphics[width=0.65\linewidth]{figures/example}%
			\caption{Exemple d'instance du Set Cover Problem et solution optimale\cite{Mount2017}}%
			\label{fig:example}%
		\end{figure}
	\section{Complexité}
		\paragraph*{}
			Le SCP est un problème d'optimisation NP-difficile, et NP-complet dans sa forme décisionnelle. IL fait notamment parti des 21 problèmes NP-complets de \citeauthor{Karp1972}~\cite{Karp1972} et est NP-complet au sens fort \cite{garey2002computers}.
		\paragraph*{}
			La démonstration de la NP-complétude du problème a été réalisée par \citeauthor{Karp1972} en \citeyear{Karp1972} dans son article \citetitle{Karp1972}\cite{Karp1972}. Dans cet article, il réalise des réductions pour 21 problèmes réputés difficiles de combinatoire et de théorie des graphes comme représenté sur la figure \ref{fig:karp_reduction_tree}.
		\begin{figure}[H]
			\centering%
			\includegraphics[width=\linewidth]{karp_reduction_tree}%
			\caption{Arbre des réductions réalisées par Karp\cite{Karp1972}}%
			\label{fig:karp_reduction_tree}%
		\end{figure}
		\paragraph*{}
			Concernant notre problème, \citeauthor{Karp1972} montre donc que le Boolean Satisfiability Problem (SATISFIABILITY) peut être réduit au Clique Problem (CLIQUE) qui peut etre réduit au Vertex/Node Cover Problem (NODE COVER) qui peut être réduit au Set Covering Problem (SET COVERING).
		\paragraph*{}
			Le théorème de Cook–Levin et sa démonstration publié en \citeyear{Cook1971} par \citeauthor{Cook1971} dans l'article \citetitle{Cook1971}\cite{Cook1971} prouve le Boolean Satisfiability Problem comme étant un problème NP-complet. Par réduction, le Set Covering Problem est donc aussi NP-complet.
		\paragraph*{}
			Pour ce qui est du Weighted Set Covering Problem (WSCP), c'est une généralisation du SCP et la réduction de ce dernier est évidente, il suffit de rajouter des poids tous égaux a une instance de SCP pour obtenir une instance de WSCP équivalente, le WSCP est donc lui aussi NP-complet.
	\section{État de l'art}
		\paragraph*{}
			Les algorithmes présentés dans cette section utilisent souvent le modèle de la relaxation lagrangienne.
			Cette relaxation consiste à supprimer des contraintes difficiles en les intégrant dans la fonction objectif en tant que pénalités.
			Les coefficients appliqués sont dans ce qu'on appelle le « vecteur multiplicateur de lagrange ».~\cite{fisher1985applications}
		\subsection{Méthodes exactes}
			\paragraph*{}
				La plupart des méthodes exactes les plus efficaces pour résoudre le problème de couverture d'ensemble sont
				des méthodes branch-and-bound
				dans lesquelles on calcule les bornes inférieures à l'aide d'une relaxation du problème en programmation
				linéaire.~\cite{caprara2000algorithms}
			\paragraph*{}
				\citeauthor{Beasley1987} a proposé un algorithme qui fonctionne de cette façon.
				Le nœud racine utilise la solution du programme linéaire relaxé, puis les bornes inférieures
				sont calculées à l'aide de la relaxation lagrangienne avec la descente de sous-gradient.~\cite{Beasley1987}
		\subsection{Méthodes approchées}
			\subsubsection{Méthodes voraces non optimales}\label{sec:soa-greedy}
				\paragraph*{}
					\citeauthor{Johnson:1973:AAC:800125.804034} a présenté dans son papier \citetitle{Johnson:1973:AAC:800125.804034}
					l'algorithme vorace standard pour le problème de couverture d'ensemble non pondéré.
					L'algorithme prend simplement le sous-ensemble qui couvre le plus
					d'éléments à chaque étape jusqu'à obtenir une solution valide. La complexité de l'algorithme est en \(O(mn)\) avec \(m\)
					le nombre d'éléments et \(n\) le nombre de sous-ensembles.~\cite{Johnson:1973:AAC:800125.804034}
					Cet algorithme n'est pas très efficace sur un problème de couverture d'ensemble pondéré car il n'en tient pas du
					tout compte.
				\paragraph*{}
					\citeauthor{Cormen:2009:IAT:1614191} ont présenté une version de l'algorithme utilisant une queue à priorité,
					permettant d'améliorer la vitesse de l'algorithme.~\cite{Cormen:2009:IAT:1614191}
				\paragraph*{}
					\citeauthor{Goldschmidt:1993:MGH:177276.177287} ont proposé une solution qui fonctionne en appliquant des
					combinaisons de plusieurs algorithmes. Des algorithmes approchés et exacts sont utilisés.
					La méthode exploite le fait qu'il est possible de trouver la solution optimale pour des ensembles de taille 2 au maximum
					en un temps polynomial. On exécute l'algorithme vorace standard de Johnson jusqu'à obtenir une taille maximale de 2 pour les
					ensembles, puis trouve la couverture optimale pour le reste, ce qui donne une solution améliorée.~\cite{Goldschmidt:1993:MGH:177276.177287}
			\subsubsection{Heuristiques}
				\paragraph*{}
					De nombreuses méthodes par heuristique sont basées sur l'observation que pour un vecteur multiplicateur de lagrange quasi-optimal
					le coût de lagrange donne une information fiable sur l'utilité générale de la sélection d'un sous-ensemble donné.
					De là, plutôt que d'utiliser les coûts originaux, on utilise les coûts de lagrange pour calculer les scores permettant
					de classer les différents sous-ensembles à sélectionner pour la solution optimale. Ces scores sont donnés
					à un algorithme vorace pour obtenir une solution valide.~\cite{caprara2000algorithms}
				\paragraph*{}
					\citeauthor{Beasley1990} a proposé un algorithme qui calcule à chaque itération de l'optimisation par sous-gradient une
					solution valide de la façon suivante : un ensemble \(S\) est initialisé avec les sous-ensembles sélectionnés
					par la solution de la relaxation lagrangienne. Puis, pour chaque point non couvert par \(S\),
					le sous ensemble avec le coût original le plus faible est ajouté à \(S\). Pour finir, les différents sous-ensembles
					de \(S\) sont considérés par ordre de coût original décroissant et on enlève le sous-ensemble \(j\) si \(S \backslash \{j\}\)
					est toujours une solution valide. À chaque itération on effectue un fixage de coût de lagrange pour réduire la taille
					du problème.~\cite{Beasley1990}	
				\paragraph*{}
					\citeauthor{jacobs1995lsh} ont proposée une approche basée sur le recuit simulé. Une solution initiale \(S\)
					est générée par un algorithme vorace qui à chaque itération sélectionne aléatoirement un point non couvert
					et ajoute à la solution le premier sous-ensemble qui couvre ce point. Après cette addition, les colonnes
					redondantes sont enlevées de la solution, et le processus est continué jusqu'à l'obtention d'une solution valide.
					Ensuite, on effectue un certain nombre d'itérations de recuit simulé : on choisi un sous-ensemble dans \(S\) aléatoirement
					et on l'enlève, puis on complète de manière vorace de manière à obtenir une autre solution valide \(S'\). Si \(S'\)
					est une meilleure solution que \(S\), \(S'\) remplace \(S\), sinon il remplace avec une certaine probabilitié qui
					décroit exponentiellement au fil des itérations.~\cite{jacobs1995lsh}
				\paragraph*{}
					\citeauthor{Brusco1999} ont ensuite proposé une amélioration à cette méthode.
					Premièrement, au lieu de choisir le sous-ensemble à enlever aléatoirement à chaque itération, on ne le fait plus
					que toutes les trois itérations. Pendant les deux itérations restantes, on choisi simplement le sous-ensemble qui
					laisse le moins de point non couvert possible. Deuxièmement, pour chaque sous-ensemble \(j\) de la solution,
					on utilise une liste de « morphs », qui sont tout simplement les autres sous-ensembles « similaires » à \(j\).
					Après avoir obtenu la solution partielle et tous les quatre sous-ennsemble ajouté pour la compléter, on itère
					sur tous les sous-ensembles de la solution partielle courante
					et on remplace chaque sous ensemble par l'un de ses « morphs » si cela améliore le ratio entre le coût actuel
					et le nombre de point couverts.~\cite{Brusco1999}
				\paragraph*{}
					\citeauthor{Afif1995} ont présenté une solution basée sur l'algorithme du flot de Ford-Fulkerson qui est un algorithme
					en temps polynomial.
					Il s'agit d'effectuer une transformation du problème en un graphe de flot à résoudre par une variante de l'algorithme
					de Ford-Fulkerson. Il est montré que le problème de couverture d'ensemble peut se réduire à un problème de flot minimal
					en temps polynomial.~\cite{Afif1995}
	\section{Représentation du problème}
		\subsection{Représentation mathématique}
			\paragraph*{}
				On reprend la notation:
				\begin{itemize}
					\item \(U = \{u_1, u_2, u_3, \dots, u_m\}\), ensemble univers composé de \(m\) points
					\item \(S = \{s_1, s_2, \dots, s_n\}\), famille de \(n\) sous-ensembles de \(U\)
				\end{itemize}
				et on pose \(M = \{1,\ldots,m\}\) et \(N = \{1,\ldots,n\}\).
			\paragraph*{Matrice d'incidence\\}
				On définit la matrice d'incidence \(A = \left(a_{i,j}\right)\) de taille \(m \times n\) avec
				\[\forall i \in M,\ \forall j \in N,\ a_{i,j} = \left\{
				    \begin{array}{ll}
				        1 & \text{si } u_i \in s_j \\
				        0 & \text{sinon}
				    \end{array}
				\right.\]
				\(a_{i,j} = 1\) signifiant donc que le point \(i\) est couvert par le sous-ensemble \(j\).
			\paragraph*{Vecteur coût\\}
				On définit le vecteur coût \(n\)-dimensionnel \(c = \left(c_j\right)\) avec \(\forall j \in N\), \(c_j\) le coût du sous ensemble \(j\).
			\paragraph*{Solution\\}
				On définit un vecteur solution comme un vecteur \(n\)-dimensionnel \(x = \left(x_j\right)\) avec
				\[\forall j \in N,\ x_j = \left\{
				    \begin{array}{ll}
				        1 & \text{si } u_i \text{ fait parti de la solution}\\
				        0 & \text{sinon}
				    \end{array}
				\right.\]
				La solution ayant comme coût \(\sum_{j \in N}{c_i x_i}\) et étant valide si
				\[\forall i \in M\ ,\sum_{j \in N}{a_{ij}x_i} \ge 1\]
		\subsection{Représentation dans notre projet}
			\paragraph*{}
				La première chose que l'on peut remarquer dans la représentation de notre problème, est que la matrice d'incidence \(A\) ainsi que le vecteur solution \(x\) ne contiennent que des valeurs booléennes (égales \(0\) ou \(1\)).
			\paragraph*{}
				La deuxième chose, qui est asse commune dans le champ de l'optimisation, et particulièrement vrai dans le cas des algorithmes que nous avons implémenté, est qu'une grande partie du temps de calcul est dédié a la vérification de la validité des solutions ainsi qu'a l'évaluation des solution par la fonction objectif.
			\paragraph*{}
				Notre fonction objective étant
				\[f(x) = \sum_{j \in N}{c_i x_i}\]
				Cette dernière peut être implémentée avec une boucle simple et sa complexité est en \(\mathcal{O}(n)\).
			\paragraph*{}
				Pour ce qui est de la vérification de la validité des solution, il faut s'assurer que:
				\[\forall i \in M\ ,\sum_{j \in N}{a_{ij}x_i} \ge 1\]
				Un algorithme évident est de faire deux boucles imbriquées, la première de longueur \(m\) sur les points et la deuxième de longueur \(n\) sur les sous-ensembles. Cet algorithme aurais alors une complexité en \(\mathcal{O}(mn)\). Une première optimisation serais d’arrêter la deuxième boucle au premier sous-ensemble qui couvre le point, cependant nous avons plutôt décider d'adopter une approche qui permet de prendre avantage des processeurs modernes (post-2013).
			\paragraph*{}
				En effet les processeurs modernes sont équipé d'instruction SIMD (Single Instruction Multiple Data), on peut notamment penser a l'AVX512 sur les processeurs Intel qui permet de réaliser des opérations sur 512 bits simultanément. Nous avons donc décider de stoker la matrice d'incidence \(A\) ainsi que le vecteur solution \(x\) sur les bits de types primitifs \Cpp{} afin de pouvoir leur appliquer des opérations simultanément. Nous allons considéré pour les paragraphes suivant un processeur 64 bits standard et le fait que les types primitifs courants sont sur 64 bits (ce qui est une simplification ne prenant pas en compte les possibles optimisations que le compilateur pourrais réaliser).
			\paragraph*{}
				Pour cela une classe \texttt{dynamic\_bitset}\cite{dynamicbitset}, implémentée par un membre du groupe, et permet d'utiliser un bitset dynamique (dont la taille n'est pas connue a la compilation) et d'appliquer toutes les opérations booléennes de façon optimisée en tirant avantage des instructions SIMD du processeur si elles sont présentes. De plus les processeur standards sont aussi équipés d'instructions pour connaitre le nombre de bits à \(1\) et la position du bit de poids le plus faible à \(1\), cela divisant donc la complexité du parcourt de \(x\) pour trouver le prochain sous-ensemble inclut dans la solution par jusqu’à 64. Et enfin le fait de stocker \(k\) information sur \(k\) bits au lieu de \(64k\), permet de faire une bien meilleur utilisation du cache processeur et nous permet de gagner énormément en performances.
			\paragraph*{}
				La matrice d'incidence \(A\) devient donc un vecteur de \(n\) bitsets de taille \(m\), un par sous-ensemble et le vecteur solution \(x\) devient un bitset de taille \(n\). La figure \ref{fig:representation_to_bitsets} illustre ce passage pour une instance a 5 points et 7 sous-ensembles.
			\begin{figure}[H]
				\centering%
				\resizebox{0.8\textwidth}{!}{\includestandalone{./figures/representation_to_bitsets}}%
				\caption{Passage de la représentation mathématique aux bitsets (instance a 5 points et 7 sous-ensembles)}%
				\label{fig:representation_to_bitsets}%
			\end{figure}
			\paragraph*{}
				Avec une telle représentation, la vérification de la validité d'une solution peut se faire efficacement en réalisant une opération booléenne \textit{OR} sur tout les bitsets \(S_i\) pour lesquels \(x_1\) est a \(1\) et en vérifiant que le bitset résultat de cette opération a tout ces bits a \(1\). La figure \ref{fig:solution_validity_check_function} représente cette opération.
			\begin{figure}[H]
				\centering%
				\resizebox{\textwidth}{!}{\includestandalone{./figures/solution_validity_check_function}}%
				\caption{Vérification de la validité d'une solution avec la représentation a base de bitsets (instance a 5 points et 7 sous-ensembles)}%
				\label{fig:solution_validity_check_function}%
			\end{figure}
			\paragraph*{}
				L'opération \textit{OR} sur les bitsets ainsi que la vérification que tout les bits sont a \(1\) est de complexité \(\mathcal{O}(1)\) pour un nombre de bits raisonnable (ce qui est largement le cas même pour les plus grandes instances que nous utiliserons). Notre algorithme de vérification de la validité d'une solution est donc en \(\mathcal{O}(n)\).
			\paragraph*{}
				La complexité des opérations sur notre problème étant donc toutes en \(\mathcal{O}(n)\), on ne s'occupera plus par la suite du nombre de points \(m\) et on dira qu'un problème avec \(m\) points et \(n\) sous-ensembles est un problème de taille \(n\).
	\section{Instances du problème}
		\subsection{OR-Library}
			\paragraph*{}
				On utilise les groupes d'instances mis a disposition par \citeauthor{OR-Library} dans son regroupement d'instances OR-Library\cite{OR-Library}. Parmis ces instances, celles de 4 à 6 proviennent l'article \citetitle{Balas1980}\cite{Balas1980} de \citeauthor{Balas1980}, celles de A à D proviennent de l'article \citetitle{Beasley1987}\cite{Beasley1987} de \citeauthor{Beasley1987} et celles de E à H proviennent de l'article \citetitle{Beasley1990}\cite{Beasley1990} de \citeauthor{Beasley1990}.
			\paragraph*{}
				Toutes les instances du problème de ces groupes on été générées en utilisant le shémas de \citeauthor{Balas1980}\cite{Balas1980} dans lequel le cout \(c_i\) de chaque colonne \(i\) est pris aléatoirement dans l'intervalle \(\llbracket0,100\rrbracket\), chaque colonne couvre au moins une ligne et chaque ligne est couverte par au moins deux clonnes.
			\paragraph*{}
			   Les propriétés de ces groupes d'instances sont décrites dans la table \ref{table:scp_problem_sets}, la densitée étant la proportion de \(1\) dans la matrice \(a_{i,j}\). La table \ref{table:orlibrary_scp_problems_optimal_solutions}, contient les valeure optimales pour les problèmes pour lesquels elle est connue.
			\begin{table}[H]
				\centering
				\input{tables/orlibrary_scp_problem_sets}
				\caption{Groupes d'instances du SCP utilisées\cite{OR-Library,Balas1980,Beasley1987,Beasley1990}}
				\label{table:scp_problem_sets}
			\end{table}
			\begin{table}[H]
				\centering
				\begin{minipage}[t]{0.45\linewidth}
					\centering
					\input{tables/orlibrary_scp_problems_optimal_solutions_1}
				\end{minipage}
				\begin{minipage}[t]{0.45\linewidth}
					\centering
					\input{tables/orlibrary_scp_problems_optimal_solutions_2}
				\end{minipage}
				\caption{Solutions optimales des instances du SCP utilisée\cite{Beasley1990}}
				\label{table:orlibrary_scp_problems_optimal_solutions}
			\end{table}
		\subsection{Instances générées}\label{sec:generated-instances}
			\paragraph*{}
				Les instances de OR-Library étant de grande taille, elles ne nous permettent pas de tester nos méthodes exactes, nous avons donc codé un générateur d'instances utilisant la même méthode qu'utilisée pour celles de OR-Library (shémas de \citeauthor{Balas1980}\cite{Balas1980}).
			\paragraph*{}
				Afin de benchmarcker les différentes méthodes implémentées, notamment les méthodes exactes avec des instances identiques pour avoir des valeures comparables, nous avons utilisé le générateur pour réaliser un ensemble d'instances de test de taille \(2\) a \(100\) avec \(200\) points.
			\paragraph*{}
				La table \ref{table:generated_scp_problems_optimal_solutions} regroupe les solutions optimales de nos instances lorsque les méthodes exactes nous ont permis de les obtenir.
			\paragraph*{}
			\begin{table}[H]
				\centering
				\begin{minipage}[t]{0.3\linewidth}
					\centering
					\input{tables/generated_scp_problems_optimal_solutions_1}
				\end{minipage}
				\begin{minipage}[t]{0.3\linewidth}
					\centering
					\input{tables/generated_scp_problems_optimal_solutions_2}
				\end{minipage}
				\begin{minipage}[t]{0.3\linewidth}
					\centering
					\input{tables/generated_scp_problems_optimal_solutions_3}
				\end{minipage}
				\caption{Solutions optimales des instances générées}
				\label{table:generated_scp_problems_optimal_solutions}
			\end{table}
	\section{Méthodes exactes}
		\subsection{Recherche exhaustive}
			\paragraph*{}
				La méthode exacte la plus simple est encore la recherche exhaustive, dans notre cas, cela consiste a tester toutes les valeurs possibles du vecteur de solution \(x\). Ce vecteur étant un vecteur booléen, il y a \(2^n\) possibilité pour un vecteur de taille \(n\) qui correspond a un problème de taille \(n\).
			\paragraph*{}
				Pour le SCP simple, il n'est pas nécessaire de tester toutes les permutations possibles, il suffit de commencer par les permutations de 1 bit, puis 2,3\ldots et dès que la permutation est une solution valide, cette dernière est la solution optimale puisque la première valide en le moins de sous-ensembles possibles.
			\paragraph*{}
				Nous avons commencé par utiliser les algorithmes classiques connu qui sont récursifs, mais ces derniers atteignent très rapidement la limite de stack du programme de par le nombre élevé d'appels récursifs pour les taille de bitset dont nous avons besoin. Ils ne sont donc pas envisageables pour notre problème.
			\paragraph*{}
				Nous avons donc réalisé un générateur de permutations G1 qui utilise une méthode s'inspirant de la programmation dynamique. En effet les permutations de \(k\) bits parmi \(n\) sont les permutations de \(k\) bits parmi \(n-1\) avec le \(n\)-ème bit a \(0\) ajouté aux permutations de \(k-1\) bits parmi \(n-1\) avec le \(n\)-ème bit a \(1\). En notant \(P_k^n\) les permutations de \(k\) bits parmi \(n\), on peut donc généré facilement \(P_k^n\) a l'aide de \(P_k^{n-1}\) et de \(P_{k-1}^{n-1}\).
			\paragraph*{}
				Le générateur G1 génère donc les \(P_i^n\) pour \(i \in \llbracket 0, n \rrbracket\) en générant successivement les \(P_i^t\) pour \(t\in \llbracket 1, n \rrbracket\) en utilisant les \(P_i^t\) de la génération précédente comme représenté sur la figure \ref{fig:g1_permutations}.
			\begin{figure}[H]
				\centering%
				\resizebox{0.4\textwidth}{!}{\includestandalone{./figures/g1_permutations}}%
				\caption{Méthode de génération des \(\forall i \in \llbracket 0, n \rrbracket,\ P_i^n\) du générateur G1}%
				\label{fig:g1_permutations}%
			\end{figure}
			\paragraph*{}
				Le problème avec générateur G1 est qu'il a besoin de stocker toutes les permutations de la génération précédente pour générer la suivante or le nombre de permutation augmente en \(2^n\) et son utilisation de RAM pour stocker les permutations dépasse les 8Go pour générer les permutations sur un bitset de taille 27 (voir figure \ref{figure:permutations_generators_ram}).
			\begin{figure}[H]
				\centering
				\input{plots/permutations_generators_ram}%
				\caption{Utilisation de la RAM par les générateurs de permutations}
				\label{figure:permutations_generators_ram}
			\end{figure}
			\paragraph*{}
				Nous avons donc réalisé un générateur G2 qui ne nécessite pas de stocker les permutation des génération précédentes mais va les construires dynamiquement au fil de la génération des \(P_i^n\), ce générateur est donc bien moins performant (voir figure \ref{figure:plots/permutations_generators_time}) mais a l'avantage de ne pas consommer excessivement de RAM (voir figure \ref{figure:permutations_generators_ram}). Le générateur G2 peut générer les \(P_i^n\) jusqu'à \(n = 32\) en moins d'une heure, ce qui est bien loin d’être suffisant pour des grandes instances tels que celles de OR-Library.
			\paragraph*{}
				Les générateurs G1 et G2 génèrent les permutations de 1 bit, puis 2,3\ldots afin d’être efficace pour le SCP simple mais il est possible de générer ces permutations de façon plus efficace si on ne restrein pas l'ordre de génération. Nous avons donc codé un générateur G3 WSCP qui est plus performant que G1 et G2 mais ne garanti pas un ordre croissant sur le nombre de bits a \(1\). G3 est donc moins performant que G1 et G2 pour le SCP mais plus performant pour le WSCP.
			\paragraph*{}
				Le générateur G3 génère les permutations en considérant le bitset comme la représentation binaire d'un nombre, il suffit de commencer a 0 et d'incrémenter le nombre représenté jusqu'à retourner a 0 pour être passé par toutes les valeurs représentables sur \(n\) bits et donc toutes les permutations. L'algorithme consiste donc à appliquer donc successivement l'algorithme d'incrément binaire qui commence par le bit de poids le plus faible et remonte les bits en transformant les \(1\) en \(0\) jusqu’à arriver a la fin du bitset ou a un \(0\) qu'il transforme en \(1\).
			\paragraph*{}
				Les 3 générateurs réalisent (en pire cas pour G3) \(\mathcal{O}(n)\) opération pour générer chaque permutation, la complexité des 3 générateurs pour générer les \(2^n\) permutations est donc en \(\mathcal{O}(n2^n)\). Pour les comparer, on réalise donc une étude empirique dont les résultats sont visibles sur les graphiques de la figure \ref{figure:plots/permutations_generators_time}. Comme prévu, le générateur G3 est le plus performant, cependant il n'est pas adapté pour le SCP simple.
			\begin{figure}[H]
				\centering
				\input{plots/permutations_generators_time_lin}\\
				\input{plots/permutations_generators_time_log}%
				\caption{Temps nécessaire pour générer toutes les permutations d'un bitset}
				\label{figure:plots/permutations_generators_time}
			\end{figure}
			\paragraph*{}
				En se fiant aux données de notre étude, sur l'ordinateur de test, la durée nécessaire au générateur G3 pour générer les permutations d'un vecteur de taille \(x\) est approximable par la fonction \(4.114961289.10^{-9}\left(e^{0.693072216x} + 1\right)\). Si l'on voulait résoudre les plus petites instances de OR-Library à \(1000\) éléments, il faudrait donc un temps théorique de \(4 \times 10^{492}\)s au générateur G3 pour générer les permutations du vecteur \(x\) et tester toutes les possibilités. Au vu de inefficacité de la recherche exhaustive nous avons décidé d'implémenter une seconde méthode exacte: le Branch-and-Bound.
		\subsection{Branch-and-Bound}
			\paragraph*{}
				Dans l'article \citetitle{caprara2000algorithms} de \citeauthor{caprara2000algorithms} on peut lire ceci:
				\begin{quote}
					The most effective exact approaches to SCP are branch-and-bound algorithms in which lower bounds are computed by solving the LP relaxation of SCP[\ldots]. In particular, all the [exact] algorithms which have been tested on the instances from the OR Library are of this type. The main reason for the success of these approaches is the fact that, despite the LP lower bound is not always very strong for these instances, it is apparently very dificult to get significantly stronger lower bounds by alternative methods which are computationally more expensive.\cite{caprara2000algorithms}
				\end{quote}
			\paragraph*{}
				Le Branch-and-Bound est donc une méthode efficace pour notre problème mais requiert un solveur de programme linéaire dans son application la plus performante. Développer un tel solveur ou l'intégrer au projet d'une source externe prenant trop de temps et s'éloignant de notre but de travailler sur des méthodes spécifiques au WSCP, nous avons décidé d'implémenter un Branch-and-Bound spécifique au problème sans solveur de programme linéaire.
			\paragraph*{}
				Notre Branch-and-Bound parcourt donc un arbre de décision où, au niveau \(i\), le choix de la valeur du bit \(i\) (\(0\) ou \(1\)) crée une séparation en 2 branches tel que représenté sur le figure \ref{fig:bnb}. Les bits dont la valeur n'a pas encore été décidés sont a \(0\) afin que les sous-ensembles auxquels ils correspondent n'interviennent pas dans le coût de la solution.
			\paragraph*{}
			\begin{figure}[H]
				\centering%
				\resizebox{0.75\textwidth}{!}{\includestandalone{./figures/bnb}}%
				\caption{Arbre parcourut par la Branch-and-Bound (sans coupures) pour un bitset de taille 3}%
				\label{fig:bnb}%
			\end{figure}
			\paragraph*{}
				La borne utilisée pour couper des branches de l'arbre est une borne maximum sur le coût de la solution. Cette borne est initialisé avec le coût de la solution trouvée par l'algorithme vorace pondéré (weighted greedy) qui sera présenté dans la section \ref{sec:approx-greedy}. Cette solution initiale est aussi utilisée comme solution optimale potentielle initiale.
			\paragraph*{}
				A chaque noeud de l'arbre la solution est évaluée, si cette dernière a une évaluation plus grande que la borne, la branche est coupé, ajouter des sous-ensembles a la solution ne pouvant qu'augmenter sont coût qui dépasse déjà celui de la borne. Si elle a une évaluation plus faible que la borne, si la solution n'est pas valide, l'algorithme continue, peut-être en ajoutant plus de sous-ensemble cela arrivera a une solution valide avec un coût toujours inférieur a la borne, si la solution est valide, sont coût devient la nouvelle borne et la solution devient la nouvelle solution optimale potentielle.
			\paragraph*{}
				La complexité moyenne d'un Branch-and-Bound est très dure a évaluer mais on peut au moins être sur que dans le pire cas la complexité est égale a celle de la recherche exhaustive et est donc en \(\mathcal{O}(n2^n)\). Nous avons donc réalisé une étude empirique afin d'avoir une idée de l'amélioration apportée par le Branch-and-Bound par rapport à la recherche exhaustive. Nous avons donc mesuré le temps de résolution de la recherche exhaustive avec le générateur G3 (le plus rapide) ainsi que avec le Branch-and-Bound, les résultats sont visibles sur les graphiques de la figure \ref{figure:plots/exhaustive_bnb_time}.
			\paragraph*{}
				Les résolutions on été réalisées sur les instances de problèmes que nous avons généré (voir section \ref{sec:generated-instances}) de la plus petite à la plus grande en arrêtant la résolution au bout de 1 heure si cette dernière n'était pas terminée (donnant des points de discontinuité dans les graphiques de la figure \ref{figure:plots/exhaustive_bnb_time}). C'est aussi lors de cette étude que le Branch-and-Bound nous a donné les résultats exacts présentés dans la section \ref{sec:generated-instances}, les résultats manquant étant ceux des instances pour lequel 1 heure n'à pas suffit a les résoudre.
			\paragraph*{}
				Les temps de résolution du Branch-and-Bound sont bien meilleurs que ceux de le recherche exhaustive, cependant sa performance dépend des instances et sans une configuration avantageuse, il ne nous sera toujours pas passible de résoudre les instances de OR-Library qui contient uniquement des problèmes de taille \(\ge 1000\). Les méthodes exactes atteignant leur limites, ils nous faut donc nous tourner du coté des méthodes approchées.
			\paragraph*{}
				\hfill{}
			\begin{figure}[H]
				\centering
				\input{plots/exhaustive_bnb_time_lin}\\
				\input{plots/exhaustive_bnb_time_log}%
				\caption{Temps nécéssaire pour résoudre un problème pour la recherche exhaustive et le Branch and bound (les résolutions trop longues ont été arreté apres 1h d'éxécution et leur valeures ne sont pas représentés)}
				\label{figure:plots/exhaustive_bnb_time}
			\end{figure}
	\section{Méthodes approchées}
		\subsection{Algorithmes voraces}\label{sec:approx-greedy}
			\paragraph*{}
				Nous avons commencé par implémenter des algorithmes voraces qui permettent de trouver des solutions approchées très rapidement.
			\paragraph*{}
				Le premier algorithme que nous avons implémenté est celui de
				\citeauthor{Johnson:1973:AAC:800125.804034}~\cite{Johnson:1973:AAC:800125.804034} présenté dans la section~\ref{sec:soa-greedy}
				où l'on intègre dans la solution les sous-ensembles couvrant le plus de point d'abord.
				On utilise le nombre de points qu'un sous-ensemble permet de couvrir de façon non redondante comme score.
			\paragraph*{}
				Pour le problème \emph{A.1} dont la solution optimale est \(253\), on obtient la solution de coût \(1444\) suivante :
			\begin{lstlisting}
[info] found unweighted greedy solution with 42 subsets cost of 1444 in 0.108285s
[info] Unweighted greedy solution: scp::Solution{
	selected subsets number = 42,
	selected subsets = 7, 32, 112, 132, 134, 136, 188, 191, 244, 245, 248, 274, 299, 392, 399, 428, 548, 816, 819, 922, 925, 960, 1042, 1068, 1096, 1161, 1208, 1271, 1300, 1302, 1412, 1418, 1454, 1505, 1565, 1971, 2056, 2154, 2341, 2462, 2752, 2941,
	cover all points = true,
	cost = 1444,
}\end{lstlisting}
			\paragraph*{}
				Pour un problème de couverture d'ensembles pondéré, cet algorithme est très mauvais.
				Nous avons donc décidé d'implémenter une version de l'algorithme qui prend en compte le coût des sous ensembles pour
				sélectionner lequel ajouter.
				Pour cela, nous utilisons le rapport \(\frac{n}{c}\) avec \(n\) le nombre de points actuellement non couverts que le
				sous-ensemble nous permettrait de couvrir et \(c\) le coût du sous-ensemble. C'est à dire le rapport de l'utilité sur
				le coût de la même façon que présenté dans le cours dans le chapitre sur les algorithmes voraces.
			\paragraph*{}
				De la même façon que précédememnt, pour le problème \emph{A.1} dont la solution optimale est \(253\), on obtient cette fois
				une solution de coût \(288\) :
			\begin{lstlisting}
[info] found weighted greedy solution with 89 subsets and cost of 288 in 0.564261s
[info] Weighted greedy solution: scp::Solution{
	selected subsets number = 89,
	selected subsets = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 27, 28, 30, 32, 33, 34, 37, 38, 39, 40, 42, 45, 46, 47, 48, 49, 50, 51, 54, 55, 59, 62, 63, 65, 66, 67, 69, 74, 76, 77, 78, 85, 86, 87, 88, 89, 90, 91, 92, 94, 97, 104, 105, 108, 112, 114, 115, 116, 117, 122, 123, 132, 137, 143, 162, 164, 172, 188, 189, 191, 214, 221, 268, 331,
	cover all points = true,
	cost = 288,
}\end{lstlisting}
			\paragraph*{}
				Utiliser le ratio utilité / coût nous permet donc d'obtenir une solution bien meilleure. On constate que dans le cas du
				problème de couverture d'ensemble pondéré, la meilleure solution n'est pas toujours celle qui utilise le moins de
				sous-ensembles. En effet, cette dernière solution utilise \(89\) solutions contre \(42\).
		\subsection{Méta-heuristiques}
			\subsubsection{Recuit simulé}
				\paragraph*{}
					TODO
			\subsubsection{Algorithme génétique}
				\paragraph*{}
					TODO
		\subsection{Analyse des méthodes approchées appliquées aux problèmes d'OR-Library}
			\paragraph*{}
				TODO

			\begin{table}[H]
				\centering
				\input{tables/orlibrary_scp_approached}
				\caption{Les solutions obtenues à l'aide des méthodes approchées}
				\label{table:approached-solutions}
			\end{table}

			\begin{table}[H]
				\centering
				\input{tables/genetic_parameters}
				\caption{Les paramètres qui nous ont permis d'obtenir la meilleure solution avec l'algorithme génétique.
					Le format est: (\(p\) \(t_r\) \(t_m\) \(t_l\) \(r_{iter}\) \(r_{tmp}\)) avec \(p\) = taille de la population,
					\(t_r\) = taux de remplacement, \(t_m\) = taux de mutations, \(t_l\) = taux de recherche locale, \(r_{iter}\) =
					le nombre d'itérations du recuit simulé, \(r_{tmp}\) = température initiale du recuit simulé.}
				\label{table:genetic-parameters}
			\end{table}

			\begin{table}[H]
				\centering
				\input{tables/simulated_annealing_parameters}
				\caption{Les paramètres qui nous ont permis d'obtenir la meilleure solution avec le recuit simulé.
					Le format est: (\(t_{init}\) \(t_{final}\)) avec \(t_{init}\) = température initiale et
					\(t_{final}\) = température finale}
				\label{table:genetic-parameters}
			\end{table}
	\section{Conclusion}
		\paragraph*{}
			En tant que problème NP complet, la méthode exacte avec le branch-and-bound peut être très rapide ou très lent selon la configuration du problème.
			Pour résoudre des problèmes de taille supérieure, il nous a fallu implémenter des méthodes approchées.
			TODO
	\newpage\printbibliography[heading=bibintoc]{}
\end{document}

% Problem set        Files
% 4                  scp41, ..., scp410
% 5                  scp51, ..., scp510
% 6                  scp61, ..., scp65
% A                  scpa1, ..., scpa5
% B                  scpb1, ..., scpb5
% C                  scpc1, ..., scpc5
% D                  scpd1, ..., scpd5
% E                  scpe1, ..., scpe5

% Problem set        Files
% E                  scpnre1, ..., scpnre5
% F                  scpnrf1, ..., scpnrf5
% G                  scpnrg1, ..., scpnrg5
% H                  scpnrh1, ..., scpnrh5
